{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20bf712e",
      "metadata": {
        "id": "20bf712e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.checkpoint import checkpoint_sequential, checkpoint\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "def sizeof_fmt(num, suffix='B'):\n",
        "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
        "        num /= 1024.0\n",
        "    return \"%.1f%s%s\" % (num, 'Yi', suffix)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = nn.Linear(10,10)\n",
        "x = torch.randn((1, 10))\n",
        "checkpoint(f, x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NtkYc4pOqMn",
        "outputId": "8d19f490-14e2-4332-d1d8-55f5ff26eeb1"
      },
      "id": "_NtkYc4pOqMn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0846,  0.6789,  0.0073,  0.0761, -1.1562, -0.7660, -0.0578,  0.3709,\n",
              "          0.1660,  0.5099]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba399d79",
      "metadata": {
        "id": "ba399d79"
      },
      "source": [
        "## Simple model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eb08634",
      "metadata": {
        "id": "1eb08634",
        "outputId": "93172cb4-51c8-4a33-9860-b1acfab6ffdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=500, out_features=500, bias=False)\n",
              "  (1): Linear(in_features=500, out_features=500, bias=False)\n",
              "  (2): Linear(in_features=500, out_features=500, bias=False)\n",
              "  (3): Linear(in_features=500, out_features=500, bias=False)\n",
              "  (4): Linear(in_features=500, out_features=500, bias=False)\n",
              "  (5): Linear(in_features=500, out_features=500, bias=False)\n",
              "  (6): Linear(in_features=500, out_features=500, bias=False)\n",
              "  (7): Linear(in_features=500, out_features=500, bias=False)\n",
              "  (8): Linear(in_features=500, out_features=500, bias=False)\n",
              "  (9): Linear(in_features=500, out_features=500, bias=False)\n",
              "  (10): Linear(in_features=500, out_features=500, bias=False)\n",
              "  (11): Linear(in_features=500, out_features=500, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nn_depth = 12\n",
        "\n",
        "list_fc = []\n",
        "\n",
        "# Build the model\n",
        "for _ in range(nn_depth):\n",
        "    list_fc.append(nn.Linear(500, 500, bias=False, device=device))\n",
        "fc_nn = nn.Sequential(*list_fc)\n",
        "fc_nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a9bab32",
      "metadata": {
        "id": "3a9bab32",
        "outputId": "7aab910b-b27f-4de1-b4b7-c5f2bfdee122",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "memory size of x: 1000448\n"
          ]
        }
      ],
      "source": [
        "save_before = torch.cuda.memory_allocated()\n",
        "x = torch.randn([500,500], device=device)\n",
        "x_mem_size = torch.cuda.memory_allocated() - save_before\n",
        "print(f\"memory size of x: {x_mem_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea758127",
      "metadata": {
        "id": "ea758127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc46152a-7d98-4e45-e8fd-8838d0a05328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "memory allocation in fwd: 11.4MiB\n"
          ]
        }
      ],
      "source": [
        "def test0():\n",
        "    # TODO: use nn_depth and x_mem_size to predict the allocation during fwd\n",
        "\n",
        "    save_before = torch.cuda.memory_allocated()\n",
        "\n",
        "    y = fc_nn(x)\n",
        "    print(f\"memory allocation in fwd: {sizeof_fmt(torch.cuda.memory_allocated() - save_before)}\")\n",
        "\n",
        "test0()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bdb2e5e",
      "metadata": {
        "id": "8bdb2e5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad1b459-e0c6-4db2-b20f-d5e7e0ea39c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "memory size of x: 1000000\n",
            "memory size of w: 1000000\n",
            "shape of y: torch.Size([500, 500])\n",
            "\n",
            "memory allocation in bwd: 9.1MiB\n",
            "max memory allocation in bwd: 10521600\n"
          ]
        }
      ],
      "source": [
        "nn_depth = 1\n",
        "batch_size = 500\n",
        "\n",
        "# Build the model\n",
        "list_fc = []\n",
        "for _ in range(nn_depth):\n",
        "    list_fc.append(nn.Linear(500, 500, bias=False, device=device))\n",
        "fc_nn = nn.Sequential(*list_fc)\n",
        "\n",
        "def test1(batch_size):\n",
        "    save_before = torch.cuda.memory_allocated()\n",
        "    x = torch.randn([batch_size,500], device=device)\n",
        "    x_mem_size = x.numel()*4\n",
        "    w_mem_size = fc_nn.get_submodule('0').weight.numel()*4\n",
        "\n",
        "    print(f\"memory size of x: {x_mem_size}\")\n",
        "    print(f\"memory size of w: {w_mem_size}\")\n",
        "\n",
        "    y = fc_nn(x)\n",
        "    print(f\"shape of y: {y.shape}\\n\")\n",
        "\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    max_before = torch.cuda.max_memory_allocated()\n",
        "    save_before = torch.cuda.memory_allocated()\n",
        "\n",
        "\n",
        "    loss = y.mean()\n",
        "    loss.backward()\n",
        "    # TODO: predict the allocation during bwd\n",
        "\n",
        "    print(f\"memory allocation in bwd: {sizeof_fmt(torch.cuda.memory_allocated() - save_before)}\")\n",
        "    # print(f\"predict memory allocation in fwd: {sizeof_fmt(predict_mem_in_bwd)}\\n\")\n",
        "\n",
        "    # TODO: predict the allocation during bwd\n",
        "    # predict_max_in_bwd = ...\n",
        "\n",
        "    print(f\"max memory allocation in bwd: {torch.cuda.max_memory_allocated() - max_before}\")\n",
        "    # print(f\"predict memory allocation in bwd: {predict_max_in_bwd}\")\n",
        "\n",
        "test1(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff844480",
      "metadata": {
        "id": "ff844480"
      },
      "source": [
        "## Torch.checkpoin examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2e5f8bc",
      "metadata": {
        "id": "e2e5f8bc"
      },
      "outputs": [],
      "source": [
        "nn_depth = 24\n",
        "batch_size = 10000\n",
        "\n",
        "# Build the model\n",
        "list_fc = []\n",
        "for _ in range(nn_depth):\n",
        "    list_fc.append(nn.Linear(500, 500, bias=False, device=device))\n",
        "fc_nn = nn.Sequential(*list_fc)\n",
        "\n",
        "x = torch.randn([batch_size,500], device=device, requires_grad=True)\n",
        "\n",
        "def test2(verbose=True):\n",
        "    x = torch.randn([batch_size,500], device=device, requires_grad=True)\n",
        "\n",
        "    save_before = torch.cuda.memory_allocated()\n",
        "    x_mem_size = x.numel()*4\n",
        "    w_mem_size = fc_nn.get_submodule('0').weight.numel()*4\n",
        "\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    max_before = torch.cuda.max_memory_allocated()\n",
        "    save_before = torch.cuda.memory_allocated()\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"memory size of x: {sizeof_fmt(x_mem_size)}\")\n",
        "        print(f\"memory size of w: {sizeof_fmt(w_mem_size)}\")\n",
        "\n",
        "    y = fc_nn(x)\n",
        "    if verbose:\n",
        "        print(f\"memory allocation in fwd: {sizeof_fmt(torch.cuda.memory_allocated() - save_before)}\\n\")\n",
        "\n",
        "    loss = y.mean()\n",
        "    loss.backward()\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"memory allocation in bwd: {sizeof_fmt(torch.cuda.memory_allocated() - save_before)}\")\n",
        "        print(f\"max memory allocation in bwd: {sizeof_fmt(torch.cuda.max_memory_allocated() - max_before)}\")\n",
        "    fc_nn.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tirch.utils."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "qciuQwKaOgrm",
        "outputId": "8d4c3ae2-3454-4470-b13e-2749cc109238"
      },
      "id": "qciuQwKaOgrm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-26-9d1e08adf879>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-9d1e08adf879>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from tirch.utils.\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3186a0f5",
      "metadata": {
        "id": "3186a0f5"
      },
      "outputs": [],
      "source": [
        "nn_depth = 24\n",
        "batch_size = 10000\n",
        "n_segments = 6# How many checkpoints to insert\n",
        "\n",
        "# Build the model\n",
        "list_fc = []\n",
        "for _ in range(nn_depth):\n",
        "    list_fc.append(nn.Linear(500, 500, bias=False, device=device))\n",
        "    list_fc.append(nn.GELU().to(device))\n",
        "fc_nn = nn.Sequential(*list_fc)\n",
        "\n",
        "x = torch.randn([batch_size,500], device=device, requires_grad=True)\n",
        "\n",
        "def test3(verbose=True):\n",
        "    x = torch.randn([batch_size,500], device=device, requires_grad=True)\n",
        "\n",
        "    save_before = torch.cuda.memory_allocated()\n",
        "    x_mem_size = x.numel()*4\n",
        "    w_mem_size = fc_nn.get_submodule('0').weight.numel()*4\n",
        "\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    max_before = torch.cuda.max_memory_allocated()\n",
        "    save_before = torch.cuda.memory_allocated()\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"memory size of x: {sizeof_fmt(x_mem_size)}\")\n",
        "        print(f\"memory size of w: {sizeof_fmt(w_mem_size)}\")\n",
        "\n",
        "    #y = checkpoint_sequential(fc_nn, n_segments, x)\n",
        "    y = x\n",
        "    for layer in fc_nn:\n",
        "      if isinstance(layer, nn.GELU):\n",
        "        y = checkpoint(layer, y)\n",
        "      else:\n",
        "        y = layer(y)\n",
        "\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"memory allocation in fwd: {sizeof_fmt(torch.cuda.memory_allocated() - save_before)}\\n\")\n",
        "\n",
        "    loss = y.mean()\n",
        "    loss.backward()\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"memory allocation in bwd: {sizeof_fmt(torch.cuda.memory_allocated() - save_before)}\")\n",
        "        print(f\"max memory allocation in bwd: {sizeof_fmt(torch.cuda.max_memory_allocated() - max_before)}\")\n",
        "    fc_nn.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee1af7db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee1af7db",
        "outputId": "a43c49a0-5438-4f5b-b06a-572bc8b04fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====original module=====\n",
            "memory size of x: 19.1MiB\n",
            "memory size of w: 976.6KiB\n",
            "memory allocation in fwd: 960.0MiB\n",
            "\n",
            "memory allocation in bwd: 62.9MiB\n",
            "max memory allocation in bwd: 1000.0MiB\n",
            "\n",
            "=====checkpoint module=====\n",
            "memory size of x: 19.1MiB\n",
            "memory size of w: 976.6KiB\n",
            "memory allocation in fwd: 960.0MiB\n",
            "\n",
            "memory allocation in bwd: 62.9MiB\n",
            "max memory allocation in bwd: 1020.0MiB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn([batch_size,500], device=device, requires_grad=True)\n",
        "\n",
        "print(\"=====original module=====\")\n",
        "test2()\n",
        "\n",
        "print()\n",
        "print(\"=====checkpoint module=====\")\n",
        "test3()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33128dd4",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "40cd558bf8bf433aa5f9f5d3764cb587",
            "0cc12c0aa3ba4b8aae8931c8edce9aa5",
            "4929914c10a14bc4a7ccad17477c2000",
            "ff16c4b9d72f413e9ce652df90d59db9",
            "d5294114a266452cb49853821ca00473",
            "7b74f138124048e3a46dbcc4e11960fb",
            "b17b1287be5345d2b4da4b538efff827",
            "c0fc966af7ad484f8db1729a27ee8e2b",
            "701f05ceed3049bfb64d37b40c406729",
            "355f4ca1cd5d454da0f20ea90dca605b",
            "7b79626a997a44c1845581a1b16359fc"
          ]
        },
        "id": "33128dd4",
        "outputId": "8c2fffde-0650-454c-86b6-afd3282937be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training time for the original module\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40cd558bf8bf433aa5f9f5d3764cb587"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(\"training time for the original module\")\n",
        "for _ in tqdm(range(100)):\n",
        "    test2(verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd2abc1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "ac2155a8b26f4dc2a71eeface88d1172",
            "2cdedf5661ad4ba4aa459d9122dbee0b",
            "1e7fae9b63fb419fac7ca621537b73df",
            "ed00a4dd2f354e239faef3a3c44b994d",
            "f56f5c81ca424568846d95faecd7da7d",
            "76a29ff25d8747fc8b3aea4657723ed8",
            "9c7e96facb80431688da8a52369a2707",
            "5e12b217c83341f6aaef8b6d81ea60b6",
            "d686e14986be454992dc7e44506adef3",
            "f3b779a666cc45e297efdb9f2387b1a4",
            "434ced32569048ffaf7b4892ae835174"
          ]
        },
        "id": "cd2abc1a",
        "outputId": "c7fed854-6fd3-4a4d-be75-d8643abcabbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training time for the checkpoint module\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac2155a8b26f4dc2a71eeface88d1172"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(\"training time for the checkpoint module\")\n",
        "for _ in tqdm(range(100)):\n",
        "    test3(verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91b7caaf",
      "metadata": {
        "id": "91b7caaf"
      },
      "source": [
        "## Understand Peak memory (Homework)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "774964e0",
      "metadata": {
        "id": "774964e0",
        "outputId": "b352fe4b-0f47-4936-fdaf-8f60f9de57ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "depth of nn: 48\n",
            "batch size: 5000\n",
            "size of x: 10.5MiB\n",
            "\n",
            "save mem after fwd: 480.0MiB\n",
            "this should be close to: 457.8MiB\n",
            "\n",
            "max mem after fwd: 480.0MiB\n",
            "this should be close to: 457.8MiB\n",
            "\n",
            "save mem after bwd:56.3MiB\n",
            "this should be close to:55.3MiB\n",
            "\n",
            "max mem after bwd: 501.0MiB\n",
            "this should be close to:0.0B\n",
            "Something is wrong with your prediction when batch_size = 5000\n"
          ]
        }
      ],
      "source": [
        "batch_size = 5000\n",
        "nn_depth = 48\n",
        "\n",
        "list_fc = []\n",
        "\n",
        "# Build the model\n",
        "for _ in range(nn_depth):\n",
        "    list_fc.append(nn.Linear(500, 500, bias=False, device=device))\n",
        "fc_nn = nn.Sequential(*list_fc)\n",
        "\n",
        "def max_prediction(nn_depth, w_mem_size, y_mem_size):\n",
        "    mem_by_layer = [0]\n",
        "    #TODO: predict memory peak of each layer\n",
        "\n",
        "    return max(mem_by_layer)\n",
        "\n",
        "def HW1(batch_size=500, nn_depth = 6, verbose=True):\n",
        "    save_before = torch.cuda.memory_allocated()\n",
        "    x = torch.randn([batch_size,500], device=device)\n",
        "    if verbose:\n",
        "        print(f\"depth of nn: {nn_depth}\")\n",
        "        print(f\"batch size: {batch_size}\")\n",
        "        print(f\"size of x: {sizeof_fmt(torch.cuda.memory_allocated()-save_before)}\\n\")\n",
        "\n",
        "    save_before = torch.cuda.memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    max_before = torch.cuda.max_memory_allocated()\n",
        "\n",
        "    # Running Forward\n",
        "    y = fc_nn(x)\n",
        "\n",
        "    y_mem_size = y.numel()*4\n",
        "    w_mem_size = fc_nn.get_submodule('0').weight.numel()*4\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"save mem after fwd: {sizeof_fmt(torch.cuda.memory_allocated()-save_before)}\")\n",
        "        print(f\"this should be close to: {sizeof_fmt(nn_depth*y_mem_size)}\\n\")\n",
        "        print(f\"max mem after fwd: {sizeof_fmt(torch.cuda.max_memory_allocated()-max_before)}\")\n",
        "        print(f\"this should be close to: {sizeof_fmt(nn_depth*y_mem_size)}\\n\")\n",
        "\n",
        "    # Running Backward\n",
        "    loss = y.mean()\n",
        "    loss.backward()\n",
        "\n",
        "    max_real = torch.cuda.max_memory_allocated()-max_before\n",
        "    max_pred = max_prediction(nn_depth, w_mem_size, y_mem_size)\n",
        "    if verbose:\n",
        "        print(f\"save mem after bwd:{sizeof_fmt(torch.cuda.memory_allocated()-save_before)}\")\n",
        "        print(f\"this should be close to:{sizeof_fmt(nn_depth*w_mem_size+y_mem_size)}\\n\")\n",
        "        print(f\"max mem after bwd: {sizeof_fmt(max_real)}\")\n",
        "        print(f\"this should be close to:{sizeof_fmt(max_pred)}\")\n",
        "\n",
        "\n",
        "    if abs(max_pred-max_real)/max_real < 0.1:\n",
        "        if verbose: print(\"Yes they are close\")\n",
        "    else:\n",
        "        print(f\"Something is wrong with your prediction when batch_size = {batch_size}\")\n",
        "    fc_nn.zero_grad(set_to_none=True)\n",
        "\n",
        "HW1(batch_size, nn_depth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53981bb1",
      "metadata": {
        "id": "53981bb1"
      },
      "outputs": [],
      "source": [
        "test_batch_sizes = [500]\n",
        "test_batch_sizes += [300, 400, 450, 480]\n",
        "test_batch_sizes += [520, 550, 600, 700]\n",
        "test_batch_sizes += [1000, 2000, 3000, 10000]\n",
        "test_batch_sizes += [10, 20, 30,100]\n",
        "\n",
        "# Howework 1: if prediction function is correct, there should be no mistake message\n",
        "for batch_size in test_batch_sizes:\n",
        "    HW1(batch_size, nn_depth, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9921044",
      "metadata": {
        "id": "d9921044"
      },
      "source": [
        "## Torch.checkpoint (Homework)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d005ed73",
      "metadata": {
        "id": "d005ed73",
        "outputId": "78118d51-9425-4acc-8227-8313013cb872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "depth of nn: 48\n",
            "batch size: 5000\n",
            "size of x: 10.5MiB\n",
            "\n",
            "save mem after fwd: 249.5MiB\n",
            "this should be close to: 0.0B\n",
            "\n",
            "max mem after fwd: 249.5MiB\n",
            "this should be close to: 0.0B\n",
            "Something is wrong with your prediction when batch_size = 5000, n_segments = 2\n",
            "\n",
            "save mem after bwd:65.8MiB\n",
            "this should be close to:64.8MiB\n",
            "\n",
            "max mem after bwd: 294.3MiB\n",
            "this should be close to:0.0B\n",
            "Something is wrong with your prediction when batch_size = 5000, n_segments = 2\n"
          ]
        }
      ],
      "source": [
        "batch_size = 5000\n",
        "nn_depth = 48\n",
        "n_segments= 2\n",
        "list_fc = []\n",
        "\n",
        "# Build the model\n",
        "for _ in range(nn_depth):\n",
        "    list_fc.append(nn.Linear(500, 500, bias=False, device=device))\n",
        "fc_nn = nn.Sequential(*list_fc)\n",
        "\n",
        "def max_prediction_fwd_checkpoint(nn_depth, w_mem_size, y_mem_size, n_segments):\n",
        "    segment_size = nn_depth//n_segments\n",
        "    #TODO: predict memory peak of each layer\n",
        "    return 0\n",
        "\n",
        "def max_prediction_bwd_checkpoint(nn_depth, w_mem_size, y_mem_size, n_segments):\n",
        "    segment_size = nn_depth//n_segments\n",
        "    mem_by_layer = [0]\n",
        "    #TODO: predict memory peak of each layer\n",
        "\n",
        "    return max(mem_by_layer)\n",
        "\n",
        "def HW2(batch_size=500, nn_depth = 48, n_segments=1, verbose=True):\n",
        "    save_before = torch.cuda.memory_allocated()\n",
        "\n",
        "    x = torch.randn([batch_size,500], device=device, requires_grad=True)\n",
        "    x_mem_size = torch.cuda.memory_allocated()-save_before\n",
        "    if verbose:\n",
        "        print(f\"depth of nn: {nn_depth}\")\n",
        "        print(f\"batch size: {batch_size}\")\n",
        "        print(f\"size of x: {sizeof_fmt(x_mem_size)}\\n\")\n",
        "\n",
        "    save_before = torch.cuda.memory_allocated()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    max_before = torch.cuda.max_memory_allocated()\n",
        "\n",
        "    # Running Forward\n",
        "    y = checkpoint_sequential(fc_nn, n_segments, x)\n",
        "\n",
        "    y_mem_size = y.numel()*4\n",
        "    w_mem_size = fc_nn.get_submodule('0').weight.numel()*4\n",
        "\n",
        "    max_real_fwd = torch.cuda.max_memory_allocated()-max_before\n",
        "    max_pred_fwd = max_prediction_fwd_checkpoint(nn_depth, w_mem_size, y_mem_size, n_segments)\n",
        "    if verbose:\n",
        "        print(f\"save mem after fwd: {sizeof_fmt(torch.cuda.memory_allocated()-save_before)}\")\n",
        "        print(f\"this should be close to: {sizeof_fmt(max_pred_fwd)}\\n\")\n",
        "        print(f\"max mem after fwd: {sizeof_fmt(max_real_fwd)}\")\n",
        "        print(f\"this should be close to: {sizeof_fmt(max_pred_fwd)}\")\n",
        "\n",
        "    if abs(max_pred_fwd-max_real_fwd)/max_real_fwd < 0.1:\n",
        "        if verbose: print(\"Yes they are close\\n\")\n",
        "    else:\n",
        "        print(f\"Something is wrong with your prediction when batch_size = {batch_size}, n_segments = {n_segments}\\n\")\n",
        "\n",
        "\n",
        "    # Running Backward\n",
        "    loss = y.mean()\n",
        "    loss.backward()\n",
        "\n",
        "    max_real_bwd = torch.cuda.max_memory_allocated()-max_before\n",
        "    max_pred_bwd = max_prediction_bwd_checkpoint(nn_depth, w_mem_size, y_mem_size, n_segments)\n",
        "    if verbose:\n",
        "        print(f\"save mem after bwd:{sizeof_fmt(torch.cuda.memory_allocated()-save_before)}\")\n",
        "        print(f\"this should be close to:{sizeof_fmt(nn_depth*w_mem_size+2*y_mem_size)}\\n\")\n",
        "        print(f\"max mem after bwd: {sizeof_fmt(max_real_bwd)}\")\n",
        "        print(f\"this should be close to:{sizeof_fmt(max_pred_bwd)}\")\n",
        "\n",
        "\n",
        "    if abs(max_pred_bwd-max_real_bwd)/max_real_bwd < 0.1:\n",
        "        if verbose: print(\"Yes they are close\")\n",
        "    else:\n",
        "        print(f\"Something is wrong with your prediction when batch_size = {batch_size}, n_segments = {n_segments}\")\n",
        "    fc_nn.zero_grad(set_to_none=True)\n",
        "\n",
        "HW2(batch_size, nn_depth, n_segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "480601e2",
      "metadata": {
        "id": "480601e2"
      },
      "outputs": [],
      "source": [
        "test_batch_sizes = [500]\n",
        "test_batch_sizes += [300, 400, 450, 480]\n",
        "test_batch_sizes += [520, 550, 600, 700]\n",
        "test_batch_sizes += [1000, 2000, 3000, 10000]\n",
        "test_batch_sizes += [10, 20, 30,100]\n",
        "\n",
        "test_n_segments = [1,2,4,6,8,12,16,24,48]\n",
        "\n",
        "# Howework 2: if prediction function is correct, there should be no mistake message\n",
        "for batch_size in test_batch_sizes:\n",
        "    for n_segments in test_n_segments:\n",
        "        HW2(batch_size, nn_depth, n_segments, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32da8421",
      "metadata": {
        "id": "32da8421"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "40cd558bf8bf433aa5f9f5d3764cb587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0cc12c0aa3ba4b8aae8931c8edce9aa5",
              "IPY_MODEL_4929914c10a14bc4a7ccad17477c2000",
              "IPY_MODEL_ff16c4b9d72f413e9ce652df90d59db9"
            ],
            "layout": "IPY_MODEL_d5294114a266452cb49853821ca00473"
          }
        },
        "0cc12c0aa3ba4b8aae8931c8edce9aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b74f138124048e3a46dbcc4e11960fb",
            "placeholder": "​",
            "style": "IPY_MODEL_b17b1287be5345d2b4da4b538efff827",
            "value": "100%"
          }
        },
        "4929914c10a14bc4a7ccad17477c2000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0fc966af7ad484f8db1729a27ee8e2b",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_701f05ceed3049bfb64d37b40c406729",
            "value": 100
          }
        },
        "ff16c4b9d72f413e9ce652df90d59db9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_355f4ca1cd5d454da0f20ea90dca605b",
            "placeholder": "​",
            "style": "IPY_MODEL_7b79626a997a44c1845581a1b16359fc",
            "value": " 100/100 [00:09&lt;00:00,  9.36it/s]"
          }
        },
        "d5294114a266452cb49853821ca00473": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b74f138124048e3a46dbcc4e11960fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b17b1287be5345d2b4da4b538efff827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0fc966af7ad484f8db1729a27ee8e2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "701f05ceed3049bfb64d37b40c406729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "355f4ca1cd5d454da0f20ea90dca605b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b79626a997a44c1845581a1b16359fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac2155a8b26f4dc2a71eeface88d1172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2cdedf5661ad4ba4aa459d9122dbee0b",
              "IPY_MODEL_1e7fae9b63fb419fac7ca621537b73df",
              "IPY_MODEL_ed00a4dd2f354e239faef3a3c44b994d"
            ],
            "layout": "IPY_MODEL_f56f5c81ca424568846d95faecd7da7d"
          }
        },
        "2cdedf5661ad4ba4aa459d9122dbee0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76a29ff25d8747fc8b3aea4657723ed8",
            "placeholder": "​",
            "style": "IPY_MODEL_9c7e96facb80431688da8a52369a2707",
            "value": "100%"
          }
        },
        "1e7fae9b63fb419fac7ca621537b73df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e12b217c83341f6aaef8b6d81ea60b6",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d686e14986be454992dc7e44506adef3",
            "value": 100
          }
        },
        "ed00a4dd2f354e239faef3a3c44b994d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3b779a666cc45e297efdb9f2387b1a4",
            "placeholder": "​",
            "style": "IPY_MODEL_434ced32569048ffaf7b4892ae835174",
            "value": " 100/100 [00:13&lt;00:00,  6.84it/s]"
          }
        },
        "f56f5c81ca424568846d95faecd7da7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76a29ff25d8747fc8b3aea4657723ed8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c7e96facb80431688da8a52369a2707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e12b217c83341f6aaef8b6d81ea60b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d686e14986be454992dc7e44506adef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3b779a666cc45e297efdb9f2387b1a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "434ced32569048ffaf7b4892ae835174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}